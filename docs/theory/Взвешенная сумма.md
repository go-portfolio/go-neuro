Это **основная операция нейрона в нейронной сети** 

---

### **1️⃣ Формула**

[
z = w_1 \cdot x_1 + w_2 \cdot x_2 + \dots + w_n \cdot x_n + b
]

* (x_1, x_2, \dots, x_n) — **входные данные** (например, пиксели картинки, значения признаков).
* (w_1, w_2, \dots, w_n) — **веса** нейрона (параметры, которые сеть подбирает во время обучения).
* (b) — **смещение (bias)**, дополнительный параметр для сдвига результата.
* (z) — **взвешенная сумма**, которую нейрон передаёт на функцию активации.

---

### **2️⃣ Зачем это нужно**

* **Веса (w_i)** показывают, **насколько важен каждый вход**.
* **Смещение (b)** позволяет нейрону **сдвигать границу принятия решения**, даже если все входы равны нулю.
* **Взвешенная сумма (z)** — это **линейная комбинация входов**, которая потом превращается в выход нейрона через функцию активации:

[
a = f(z)
]

где (f) — функция активации (ReLU, Sigmoid, Tanh и т.д.).

---

### **Пример**

Если у нас два входа (x_1=2), (x_2=3), веса (w_1=0.5), (w_2=1), а смещение (b=0.1):

[
z = 0.5 \cdot 2 + 1 \cdot 3 + 0.1 = 1 + 3 + 0.1 = 4.1
]

Затем на (z=4.1) применяется функция активации, например ReLU: (a = \max(0, z) = 4.1).
