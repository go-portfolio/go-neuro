### 1. **Что такое функция активации**

* Функция активации решает, как нейрон будет реагировать на входные данные.
* Она вводит **нелинейность** в модель. Без неё сеть превращается просто в линейную комбинацию, и даже с большим числом слоёв она **не сможет моделировать сложные зависимости**.

---

### 2. **Почему выбирают разные функции для разных слоёв**

1. **ReLU (Rectified Linear Unit)**

   * Формула: `f(x) = max(0, x)`
   * Плюсы: простая, быстро обучается, хорошо работает в глубоких сетях.
   * Минусы: может «умирать» (нейроны перестают обновляться, если всегда дают 0).
   * Используется почти везде в скрытых слоях.

2. **Leaky ReLU**

   * Вариант ReLU, который пропускает немного отрицательные значения, чтобы нейроны не «умирали».
   * Полезна в глубоких сетях, где ReLU может быть нестабильной.

3. **Tanh**

   * Формула: сигнальная кривая от -1 до 1.
   * Плюсы: центрирована вокруг 0 → обучение иногда стабильнее.
   * Минусы: может страдать от «затухающего градиента» (градиенты становятся слишком маленькими).
   * Используется, когда нужна симметрия данных вокруг 0.

4. **Sigmoid**

   * Формула: сигмоидальная кривая от 0 до 1.
   * Плюсы: хорошо подходит для выхода, если нужна вероятность (например, классификация).
   * Минусы: градиенты могут исчезать → проблемы в глубоких сетях.
   * Обычно используется только на выходном слое для бинарной классификации.

---

### 3. **Главная идея**

* **Функция активации определяет, как нейрон «решает», что важное, а что нет.**
* Неправильный выбор функции активации → обучение замедляется, сеть плохо сходится, появляются «мертвые» нейроны или переобучение.
* Разные слои могут требовать разных функций: например, скрытые слои — ReLU, выходной слой — Sigmoid.