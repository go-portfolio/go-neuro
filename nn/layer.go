package nn

// ========================
// Структура слоя нейросети
// ========================
type Layer struct {
	In      int           // количество входов слоя (размер предыдущего слоя)
	Out     int           // количество нейронов в этом слое
	Weights [][]float64   // матрица весов: [Out][In] (каждый нейрон имеет свой набор весов)
	Biases  []float64     // смещения (bias) каждого нейрона
	Z       []float64     // взвешенная сумма входов + смещение (до активации) для каждого нейрона
	A       []float64     // выход нейронов после активации (активации)
}

// ========================
// Создание нового слоя
// ========================
func NewLayer(in, out int) *Layer {
	L := &Layer{
		In:      in,                  // количество входов
		Out:     out,                 // количество нейронов
		Weights: make([][]float64, out), // создаём пустую матрицу весов для каждого нейрона
		Biases:  make([]float64, out),   // создаём смещения для каждого нейрона
		Z:       make([]float64, out),   // массив для хранения z = w*x + b
		A:       make([]float64, out),   // массив для хранения активаций
	}

	// Инициализация весов и смещений случайными числами
	for i := 0; i < out; i++ { // перебираем каждый нейрон слоя
		L.Weights[i] = make([]float64, in) // создаём массив весов для входов этого нейрона
		for j := 0; j < in; j++ {         // перебираем каждый вход нейрона
			L.Weights[i][j] = RandomFloat() // случайная инициализация веса
		}
		L.Biases[i] = RandomFloat() // случайная инициализация смещения нейрона
	}

	return L
}

// ========================
// Прямой проход (forward pass) слоя
// ========================
func (L *Layer) Forward(x []float64) []float64 {
	// x — входные значения: 
	// - если первый слой → это входные переменные сети
	// - если скрытый слой → выходы предыдущего слоя
	for i := 0; i < L.Out; i++ { // для каждого нейрона слоя
		sum := L.Biases[i] // начинаем с смещения
		for j := 0; j < L.In; j++ { // проходим по каждому входу
			sum += L.Weights[i][j] * x[j] // суммируем взвешенные входы
		}
		L.Z[i] = sum       // сохраняем значение до активации (z)
		L.A[i] = Sigmoid(sum) // применяем функцию активации (sigmoid)
	}

	// возвращаем копию активаций слоя (не ссылку, чтобы не менять данные снаружи)
	return append([]float64{}, L.A...)
}

/*
Обозначение слоев:
- Входной слой (input layer):
  - В нашей сети он формируется из входных переменных, не имеет своих весов
- Скрытые слои (hidden layers):
  - Содержат веса, смещения, z и A
  - L.In = размер предыдущего слоя, L.Out = количество нейронов скрытого слоя
- Выходной слой (output layer):
  - То же, что скрытый слой, только L.Out = количество выходов задачи
- Forward проход:
  - Для скрытых и выходного слоя один и тот же алгоритм: z = w*x + b → A = sigmoid(z)
*/
