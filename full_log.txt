=== Training Start ===
Epoch 0 — loss=0.761701
Epoch 200 — loss=0.294044
Epoch 400 — loss=0.292995
Epoch 600 — loss=0.291957
Epoch 800 — loss=0.290925
Epoch 1000 — loss=0.289619
Epoch 1200 — loss=0.291239
Epoch 1400 — loss=0.286775
Epoch 1600 — loss=0.285134
Epoch 1800 — loss=0.283250
Epoch 2000 — loss=0.281152
Epoch 2200 — loss=0.339042
Epoch 2400 — loss=0.278635
Epoch 2600 — loss=0.277070
Ранняя остановка на эпохе 2745 (нет улучшений 500 эпох)
Training done!


=== Sample 0: Input=[0 0] ===

--- Layer 0 (Input Layer, neurons=2) ---
Layer 0 outputs (A): [0 0]

--- Layer 1 (neurons=10) ---
Neuron 0: Weights=[0.11727630470582238 -0.38159150769644923], Bias=-0.2647, Z=-0.2647 → A=-0.0026
Neuron 1: Weights=[-0.15897923682794496 1.3040949263718833], Bias=-0.2368, Z=-0.2368 → A=-0.0024
Neuron 2: Weights=[-2.952897256342078 0.46172326803729397], Bias=-0.7682, Z=-0.7682 → A=-0.0077
Neuron 3: Weights=[-2.674602175099326 -0.5579493297257786], Bias=-0.2699, Z=-0.2699 → A=-0.0027
Neuron 4: Weights=[2.159521564411095 -1.4992413907480626], Bias=-0.7256, Z=-0.7256 → A=-0.0073
Neuron 5: Weights=[-1.5525338480685362 0.10240302009979815], Bias=-0.2777, Z=-0.2777 → A=-0.0028
Neuron 6: Weights=[-0.6545299207356409 -0.4416750257313487], Bias=-0.0507, Z=-0.0507 → A=-0.0005
Neuron 7: Weights=[0.01764778185815898 -0.7293830995947088], Bias=-0.4106, Z=-0.4106 → A=-0.0041
Neuron 8: Weights=[-0.7159832404483022 -0.011422743222616], Bias=-0.0813, Z=-0.0813 → A=-0.0008
Neuron 9: Weights=[0.08128176457163765 -0.5628679368754044], Bias=-0.1554, Z=-0.1554 → A=-0.0016
Layer 1 outputs (A): [-0.0026474752358837555 -0.0023675132520286847 -0.007681533012244809 -0.0026991134922721544 -0.007256024243137534 -0.0027770029866762812 -0.0005074368330765973 -0.004105817321048571 -0.0008131225295775213 -0.0015536108174193783]

--- Layer 2 (neurons=3) ---
Neuron 0: Weights=[0.01272402759840902 -0.20519249547820467 -1.3249809374369235 -1.8576275421011377 0.7392398785174212 -1.494083055110129 -0.20645328731963453 0.30025800178649037 -0.6832084514258763 0.1775230763817624], Bias=0.0324, Z=0.0460 → A=0.0460
Neuron 1: Weights=[0.1088452964650412 0.15328196035378608 -0.6226758384111041 -0.8117263883698359 0.10460585516047656 -0.5144977816117339 0.3476017240306975 0.3450348725176234 0.11351601146192386 0.5711829734557877], Bias=-0.2755, Z=-0.2711 → A=-0.0027
Neuron 2: Weights=[0.28801041496845947 0.8853668872837126 -1.6325714337709805 -1.7435520669193019 0.7020991809699013 -0.7060081061612302 -0.7881886524607478 -0.288734886048251 -0.331366114350064 -0.3694943197602202], Bias=0.1185, Z=0.1322 → A=0.1322
Layer 2 outputs (A): [0.045975651009162725 -0.0027106072038119294 0.13218644685041403]

Network Output:
 Output neuron 0: 0.0460 (rounded 0), Target: 0
 Output neuron 1: -0.0027 (rounded 0), Target: 0
 Output neuron 2: 0.1322 (rounded 0), Target: 0

=== Sample 1: Input=[0 1] ===

--- Layer 0 (Input Layer, neurons=2) ---
Layer 0 outputs (A): [0 1]

--- Layer 1 (neurons=10) ---
Neuron 0: Weights=[0.11727630470582238 -0.38159150769644923], Bias=-0.2647, Z=-0.6463 → A=-0.0065
Neuron 1: Weights=[-0.15897923682794496 1.3040949263718833], Bias=-0.2368, Z=1.0673 → A=1.0673
Neuron 2: Weights=[-2.952897256342078 0.46172326803729397], Bias=-0.7682, Z=-0.3064 → A=-0.0031
Neuron 3: Weights=[-2.674602175099326 -0.5579493297257786], Bias=-0.2699, Z=-0.8279 → A=-0.0083
Neuron 4: Weights=[2.159521564411095 -1.4992413907480626], Bias=-0.7256, Z=-2.2248 → A=-0.0222
Neuron 5: Weights=[-1.5525338480685362 0.10240302009979815], Bias=-0.2777, Z=-0.1753 → A=-0.0018
Neuron 6: Weights=[-0.6545299207356409 -0.4416750257313487], Bias=-0.0507, Z=-0.4924 → A=-0.0049
Neuron 7: Weights=[0.01764778185815898 -0.7293830995947088], Bias=-0.4106, Z=-1.1400 → A=-0.0114
Neuron 8: Weights=[-0.7159832404483022 -0.011422743222616], Bias=-0.0813, Z=-0.0927 → A=-0.0009
Neuron 9: Weights=[0.08128176457163765 -0.5628679368754044], Bias=-0.1554, Z=-0.7182 → A=-0.0072
Layer 1 outputs (A): [-0.006463390312848248 1.0673436011690147 -0.0030643003318718692 -0.00827860678952994 -0.022248438150618163 -0.0017529727856782998 -0.004924187090390084 -0.011399648316995659 -0.0009273499618036812 -0.007182290186173422]

--- Layer 2 (neurons=3) ---
Neuron 0: Weights=[0.01272402759840902 -0.20519249547820467 -1.3249809374369235 -1.8576275421011377 0.7392398785174212 -1.494083055110129 -0.20645328731963453 0.30025800178649037 -0.6832084514258763 0.1775230763817624], Bias=0.0324, Z=-0.1841 → A=-0.0018
Neuron 1: Weights=[0.1088452964650412 0.15328196035378608 -0.6226758384111041 -0.8117263883698359 0.10460585516047656 -0.5144977816117339 0.3476017240306975 0.3450348725176234 0.11351601146192386 0.5711829734557877], Bias=-0.2755, Z=-0.1152 → A=-0.0012
Neuron 2: Weights=[0.28801041496845947 0.8853668872837126 -1.6325714337709805 -1.7435520669193019 0.7020991809699013 -0.7060081061612302 -0.7881886524607478 -0.288734886048251 -0.331366114350064 -0.3694943197602202], Bias=0.1185, Z=1.0768 → A=1.0768
Layer 2 outputs (A): [-0.0018413505492129157 -0.0011522968950292908 1.0768200950289595]

Network Output:
 Output neuron 0: -0.0018 (rounded 0), Target: 1
 Output neuron 1: -0.0012 (rounded 0), Target: 0
 Output neuron 2: 1.0768 (rounded 1), Target: 1

=== Sample 2: Input=[1 0] ===

--- Layer 0 (Input Layer, neurons=2) ---
Layer 0 outputs (A): [1 0]

--- Layer 1 (neurons=10) ---
Neuron 0: Weights=[0.11727630470582238 -0.38159150769644923], Bias=-0.2647, Z=-0.1475 → A=-0.0015
Neuron 1: Weights=[-0.15897923682794496 1.3040949263718833], Bias=-0.2368, Z=-0.3957 → A=-0.0040
Neuron 2: Weights=[-2.952897256342078 0.46172326803729397], Bias=-0.7682, Z=-3.7211 → A=-0.0372
Neuron 3: Weights=[-2.674602175099326 -0.5579493297257786], Bias=-0.2699, Z=-2.9445 → A=-0.0294
Neuron 4: Weights=[2.159521564411095 -1.4992413907480626], Bias=-0.7256, Z=1.4339 → A=1.4339
Neuron 5: Weights=[-1.5525338480685362 0.10240302009979815], Bias=-0.2777, Z=-1.8302 → A=-0.0183
Neuron 6: Weights=[-0.6545299207356409 -0.4416750257313487], Bias=-0.0507, Z=-0.7053 → A=-0.0071
Neuron 7: Weights=[0.01764778185815898 -0.7293830995947088], Bias=-0.4106, Z=-0.3929 → A=-0.0039
Neuron 8: Weights=[-0.7159832404483022 -0.011422743222616], Bias=-0.0813, Z=-0.7973 → A=-0.0080
Neuron 9: Weights=[0.08128176457163765 -0.5628679368754044], Bias=-0.1554, Z=-0.0741 → A=-0.0007
Layer 1 outputs (A): [-0.0014747121888255316 -0.003957305620308135 -0.03721050557566559 -0.029445135243265416 1.4339191400973417 -0.018302341467361646 -0.007052736040433006 -0.003929339502466982 -0.007972954934060543 -0.0007407931717030019]

--- Layer 2 (neurons=3) ---
Neuron 0: Weights=[0.01272402759840902 -0.20519249547820467 -1.3249809374369235 -1.8576275421011377 0.7392398785174212 -1.494083055110129 -0.20645328731963453 0.30025800178649037 -0.6832084514258763 0.1775230763817624], Bias=0.0324, Z=1.2301 → A=1.2301
Neuron 1: Weights=[0.1088452964650412 0.15328196035378608 -0.6226758384111041 -0.8117263883698359 0.10460585516047656 -0.5144977816117339 0.3476017240306975 0.3450348725176234 0.11351601146192386 0.5711829734557877], Bias=-0.2755, Z=-0.0749 → A=-0.0007
Neuron 2: Weights=[0.28801041496845947 0.8853668872837126 -1.6325714337709805 -1.7435520669193019 0.7020991809699013 -0.7060081061612302 -0.7881886524607478 -0.288734886048251 -0.331366114350064 -0.3694943197602202], Bias=0.1185, Z=1.2559 → A=1.2559
Layer 2 outputs (A): [1.2301367970085695 -0.0007489896933858899 1.255946990818599]

Network Output:
 Output neuron 0: 1.2301 (rounded 1), Target: 1
 Output neuron 1: -0.0007 (rounded 0), Target: 0
 Output neuron 2: 1.2559 (rounded 1), Target: 1

=== Sample 3: Input=[1 1] ===

--- Layer 0 (Input Layer, neurons=2) ---
Layer 0 outputs (A): [1 1]

--- Layer 1 (neurons=10) ---
Neuron 0: Weights=[0.11727630470582238 -0.38159150769644923], Bias=-0.2647, Z=-0.5291 → A=-0.0053
Neuron 1: Weights=[-0.15897923682794496 1.3040949263718833], Bias=-0.2368, Z=0.9084 → A=0.9084
Neuron 2: Weights=[-2.952897256342078 0.46172326803729397], Bias=-0.7682, Z=-3.2593 → A=-0.0326
Neuron 3: Weights=[-2.674602175099326 -0.5579493297257786], Bias=-0.2699, Z=-3.5025 → A=-0.0350
Neuron 4: Weights=[2.159521564411095 -1.4992413907480626], Bias=-0.7256, Z=-0.0653 → A=-0.0007
Neuron 5: Weights=[-1.5525338480685362 0.10240302009979815], Bias=-0.2777, Z=-1.7278 → A=-0.0173
Neuron 6: Weights=[-0.6545299207356409 -0.4416750257313487], Bias=-0.0507, Z=-1.1469 → A=-0.0115
Neuron 7: Weights=[0.01764778185815898 -0.7293830995947088], Bias=-0.4106, Z=-1.1223 → A=-0.0112
Neuron 8: Weights=[-0.7159832404483022 -0.011422743222616], Bias=-0.0813, Z=-0.8087 → A=-0.0081
Neuron 9: Weights=[0.08128176457163765 -0.5628679368754044], Bias=-0.1554, Z=-0.6369 → A=-0.0064
Layer 1 outputs (A): [-0.0052906272657900245 0.9083643643410698 -0.03259327289529265 -0.0350246285405232 -0.0006532225065072095 -0.017278311266363665 -0.011469486297746493 -0.01122317049841407 -0.008087182366286704 -0.006369472540457046]

--- Layer 2 (neurons=3) ---
Neuron 0: Weights=[0.01272402759840902 -0.20519249547820467 -1.3249809374369235 -1.8576275421011377 0.7392398785174212 -1.494083055110129 -0.20645328731963453 0.30025800178649037 -0.6832084514258763 0.1775230763817624], Bias=0.0324, Z=-0.0171 → A=-0.0002
Neuron 1: Weights=[0.1088452964650412 0.15328196035378608 -0.6226758384111041 -0.8117263883698359 0.10460585516047656 -0.5144977816117339 0.3476017240306975 0.3450348725176234 0.11351601146192386 0.5711829734557877], Bias=-0.2755, Z=-0.0917 → A=-0.0009
Neuron 2: Weights=[0.28801041496845947 0.8853668872837126 -1.6325714337709805 -1.7435520669193019 0.7020991809699013 -0.7060081061612302 -0.7881886524607478 -0.288734886048251 -0.331366114350064 -0.3694943197602202], Bias=0.1185, Z=1.0645 → A=1.0645
Layer 2 outputs (A): [-0.00017088884871348055 -0.0009168927554814508 1.0645473171635633]

Network Output:
 Output neuron 0: -0.0002 (rounded 0), Target: 0
 Output neuron 1: -0.0009 (rounded 0), Target: 1
 Output neuron 2: 1.0645 (rounded 1), Target: 1

=== Predictions for XOR ===
Input: [0 0] → Output: 0.0460 (rounded 0), Target: 0
Input: [0 1] → Output: -0.0018 (rounded 0), Target: 1
Input: [1 0] → Output: 1.2301 (rounded 1), Target: 1
Input: [1 1] → Output: -0.0002 (rounded 0), Target: 0

=== Predictions for AND ===
Input: [0 0] → Output: -0.0027 (rounded 0), Target: 0
Input: [0 1] → Output: -0.0012 (rounded 0), Target: 0
Input: [1 0] → Output: -0.0007 (rounded 0), Target: 0
Input: [1 1] → Output: -0.0009 (rounded 0), Target: 1

=== Predictions for OR ===
Input: [0 0] → Output: 0.1322 (rounded 0), Target: 0
Input: [0 1] → Output: 1.0768 (rounded 1), Target: 1
Input: [1 0] → Output: 1.2559 (rounded 1), Target: 1
Input: [1 1] → Output: 1.0645 (rounded 1), Target: 1
